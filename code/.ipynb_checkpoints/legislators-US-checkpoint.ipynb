{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib.request as requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory\n",
    "os.chdir(\"/Users/mathiasrask/Desktop/kandidat/E2020/AQM/\")\n",
    "\n",
    "# define url\n",
    "url = \"https://theunitedstates.io/congress-legislators/\"\n",
    "\n",
    "# define name of files to be parsed\n",
    "social_media = 'legislators-social-media.yaml'\n",
    "historical   = 'legislators-historical.yaml'\n",
    "current      = 'legislators-current.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump file in YAML-format \n",
    "#soup = bs(content)\n",
    "#with open('social-media-accounts/legislators-historical.yaml', 'w') as file:\n",
    "#    documents = yaml.dump(soup, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml(url):\n",
    "    url = requests.urlopen(url)               # connect to url\n",
    "    content = url.read().decode('utf-8')      # read and decode to UTF-8 format \n",
    "    return content\n",
    "\n",
    "def clean_yaml(string, file:str):\n",
    "    content = string.split('\\n')\n",
    "    content = [x.lstrip(\"' '|>-\") for x in content]\n",
    "    \n",
    "    if file == 'social-media':\n",
    "        content = content[18:]\n",
    "    if file == 'legislators':\n",
    "        content[0] = re.sub('[ b\\'-]', \"\", content[0])\n",
    "    return content\n",
    "\n",
    "def split_list(match:list, string, within_legislator=False):\n",
    "    def match_function():\n",
    "        matches = [re.match(match, x) for x in string]\n",
    "        matches = [x is not None for x in matches]    \n",
    "        return np.cumsum(matches).tolist()\n",
    "    \n",
    "    indices = match_function()\n",
    "    \n",
    "    def split_indices():\n",
    "        results = []\n",
    "        for idx, val in enumerate(indices):\n",
    "            if idx == 0:\n",
    "                results.append([val])\n",
    "                continue\n",
    "            if val > indices[idx - 1]:\n",
    "                results.append([val])\n",
    "            else:\n",
    "                results[-1].append(val)\n",
    "        return np.cumsum([len(x) for x in results]).tolist()\n",
    "    \n",
    "    #if within_legislator == False:\n",
    "    #    indices = split_indices()\n",
    "    #    indices = np.cumsum([len(x) for x in indices]).tolist()\n",
    "    #if within_legislator == True:\n",
    "    #    indices = split_indices()\n",
    "    #    indices = np.cumsum([len(x) for x in indices]).tolist()\n",
    "    indices = split_indices()\n",
    "    def decomposition():\n",
    "        decomposed_list = []\n",
    "        for idx, val in enumerate(indices):\n",
    "            if idx == 0:\n",
    "                decomposed_list.append(string[0:val])\n",
    "            else:\n",
    "                current  = indices[idx]\n",
    "                previous = indices[idx-1] \n",
    "                decomposed_list.append(string[previous:current])\n",
    "        return decomposed_list\n",
    "    \n",
    "    final = decomposition()\n",
    "    \n",
    "    # remove potential empty elements\n",
    "    final = [[x for x in l if x] for l in final]\n",
    "    \n",
    "    return final\n",
    "\n",
    "def get_index_positions(list_object, element):\n",
    "    \"\"\"\n",
    "    Returns the indexes of all occurrences of an element in\n",
    "    a list.\n",
    "    \"\"\"  \n",
    "    index_pos_list = []\n",
    "    index_pos = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Search for item in list from indexPos to the end of list\n",
    "            index_pos = list_object.index(element, index_pos)\n",
    "            # Add the index position in list\n",
    "            index_pos_list.append(index_pos)\n",
    "            index_pos += 1\n",
    "        except ValueError as e:\n",
    "            break\n",
    "    return index_pos_list\n",
    "\n",
    "def del_list_numpy(l, id_to_del):\n",
    "    \"\"\"\n",
    "    Delete indicies from list using numpy's delete-function.\n",
    "    \"\"\"\n",
    "    arr = np.array(l)\n",
    "    return list(np.delete(arr, id_to_del))\n",
    "\n",
    "\n",
    "def delete_party_affiliations(list_object):\n",
    "    for idx, val in enumerate(list_object):\n",
    "        \n",
    "        party_aff = [re.match('party_affiliations:',x) is not None for x in list_object[idx]['terms:']]\n",
    "        type_     = [re.match('type:',x) is not None for x in list_object[idx]['terms:']]\n",
    "        \n",
    "        type_indices      = get_index_positions(type_, True)\n",
    "        party_aff_indices = get_index_positions(party_aff, True)\n",
    "        \n",
    "        distances = []\n",
    "        indices = []\n",
    "        \n",
    "        for idx1, val1 in enumerate(party_aff_indices):\n",
    "            temp = []\n",
    "            for idx2, val2 in enumerate(type_indices):\n",
    "                temp.append(val2 - val1)\n",
    "            \n",
    "            if max(temp) > 0:\n",
    "                index = temp.index(min([i for i in temp if i > 0]))\n",
    "                indices = indices + list(range(val1, type_indices[index]))\n",
    "                distances.append(index)\n",
    "            \n",
    "            if max(temp) < 0:\n",
    "                indices = indices + list(range(max(party_aff_indices), len(list_object[idx]['terms:'])))\n",
    "        \n",
    "        list_object[idx]['terms:'] = del_list_numpy(list_object[idx]['terms:'], indices)    \n",
    "    \n",
    "    return list_object\n",
    "\n",
    "def clean_bio(list_object, element:str):    \n",
    "    \"\"\"\n",
    "    Function to clean-up fec-numbers and id to biographical information\n",
    "    \"\"\"\n",
    "    for idx, val in enumerate(list_object):\n",
    "        index = [re.match(element,x) for x in val]\n",
    "\n",
    "        if np.sum([x!=None for x in index]) > 0:\n",
    "            element_index = [x!=None for x in index].index(True)\n",
    "            if element == 'fec:':\n",
    "                matches = [re.match('[A-Z0-9]{9}', val[i]) for i in range(element_index+0, element_index+4)]\n",
    "\n",
    "            if element == 'bioguide_previous:':\n",
    "                matches = [re.match('[A-Z0-9]{7}', val[i]) for i in range(element_index+0, element_index+4)]\n",
    "\n",
    "            index_match = get_index_positions([x!=None for x in matches], True)\n",
    "\n",
    "            N_matches = np.sum([x is not None for x in matches])\n",
    "\n",
    "            if element == 'fec:':\n",
    "                element_numbers = [matches[index_match[i]].string for i in range(N_matches)]\n",
    "                element_numbers = '|'.join(element_numbers)\n",
    "                list_object[idx][element_index] = list_object[idx][element_index] + ' ' + element_numbers\n",
    "\n",
    "            if element == 'bioguide_previous:':\n",
    "                for i in range(N_matches):\n",
    "                    list_object[idx][element_index] = list_object[idx][element_index] + ' ' + matches[index_match[i]].string\n",
    "\n",
    "            list_object[idx] = del_list_numpy(list_object[idx], [i+element_index for i in index_match])\n",
    "    return list_object\n",
    "\n",
    "\n",
    "def get_row_indices(list_object, match:str):\n",
    "    return np.cumsum([re.match(match,x[0]) is not None for x in list_object])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to move bioguide into the second element of the list\n",
    "def clean_bioguide():\n",
    "    # For-loop to make sure that bioguide is at top\n",
    "    #for idx, val in enumerate(between_legislator):\n",
    "    matches = [re.match('bioguide: (.*)', x) for x in val]\n",
    "    boolean = [x is not None for x in matches]\n",
    "\n",
    "    bioguide = [x.string for x in matches if x is not None][0]\n",
    "    bio_position = get_index_positions(boolean, True)[0]\n",
    "\n",
    "    if bio_position != 1:\n",
    "        replace_element = between_legislator[idx][1]\n",
    "        between_legislator[idx][1] = bioguide\n",
    "        between_legislator[idx][bio_position] = replace_element\n",
    "    return\n",
    "\n",
    "# Function to assure that \"name:\" only figures once\n",
    "def clean_family():\n",
    "    matches = [re.match(\"name:\", x) for x in val]\n",
    "    indices = get_index_positions([x!=None for x in matches], True)\n",
    "\n",
    "    # get indices for the actual name element\n",
    "    name_indices = get_index_positions([x.group()==x.string for x in matches if x is not None], False)\n",
    "\n",
    "    # keep only indices to be changed\n",
    "    indices = [indices[x] for x in name_indices]\n",
    "\n",
    "    # construct pairs of the name-title and the name of the fam. member\n",
    "    match_pairs = [re.match('(.*): (.*)', between_legislator[idx][x]).groups() for x in indices]\n",
    "    \n",
    "    # replace family names\n",
    "    for i,v in enumerate(indices):\n",
    "        between_legislator[idx][v] = f\"{match_pairs[i][0]}_fam: {match_pairs[i][1]}\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Legislators\n",
    "\n",
    "In this section, I prepare the data for historical legislators, that is legislators not in the current US Congress (116th). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yaml-file for historical data\n",
    "content = read_yaml(url=f\"{url}{historical}\")\n",
    "content = clean_yaml(content, file='legislators')\n",
    "\n",
    "# Split list into sub-lists for each legislator\n",
    "between_legislator = split_list(match=\"id:\", string=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure bioguide is on top and clean the family section\n",
    "for idx, val in enumerate(between_legislator):\n",
    "    clean_bioguide()\n",
    "    clean_family()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete contact information from list\n",
    "delete_info = [[re.match('address:|phone:|fax:|contact_form:|office:|other_names:|rss_url:',x) for x in l] for l in between_legislator]\n",
    "boolean = [[x is None for x in c] for c in delete_info]\n",
    "delete_indices = [get_index_positions(x, False) for x in boolean]\n",
    "between_legislator = [del_list_numpy(between_legislator[idx], delete_indices[idx]) for idx in range(len(between_legislator))]\n",
    "\n",
    "# Clean up fec and bioguide \n",
    "between_legislator = clean_bio(between_legislator, element='fec:')\n",
    "between_legislator = clean_bio(between_legislator, element='bioguide_previous:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a list for each type of information for each legislator.\n",
    "# I delete party affiliations within the 'terms'-list to avoid complications.\n",
    "#within_legislator = [split_list(match=\"id:|name:|bio:|terms:|leadership_roles:|family:\", string=x) for x in between_legislator]\n",
    "within_legislator = [split_list(match=\"id:|name:|bio:|terms:|leadership_roles:|family:\", string=x) for x in between_legislator]\n",
    "within_legislator = [{x[0]:x[1:] for x in l} for l in within_legislator]\n",
    "within_legislator = delete_party_affiliations(within_legislator)\n",
    "\n",
    "# Constuct column-row pairs for each legislator\n",
    "for idx0, val0 in enumerate(within_legislator):\n",
    "    for idx1, val1 in enumerate(val0):\n",
    "        within_legislator[idx0][val1] = [re.match('(.*): (.*)', x).groups() for x in within_legislator[idx0][val1]] \n",
    "        \n",
    "for idx in range(len(within_legislator)):\n",
    "    boolean_end = [re.match('end', x) is not None for x,y in within_legislator[idx]['name:']]\n",
    "    if sum(boolean_end) > 0:\n",
    "        end_position = get_index_positions(boolean_end, True)[0]\n",
    "        del within_legislator[idx]['name:'][end_position]\n",
    "\n",
    "for idx in range(len(within_legislator)):\n",
    "    boolean_middle = [re.match('~', y) is not None for x,y in within_legislator[idx]['name:']]\n",
    "    if sum(boolean_middle) > 0:\n",
    "        middle_position = get_index_positions(boolean_middle, True)[0]\n",
    "        del within_legislator[idx]['name:'][middle_position]\n",
    "\n",
    "for idx, val in enumerate(within_legislator):\n",
    "    last_positions = get_index_positions([re.match('last', x) is not None for x,y in within_legislator[idx]['name:']],True)\n",
    "    if len(last_positions) > 1:\n",
    "        max_last = max(last_positions)\n",
    "        del within_legislator[idx]['name:'][max_last]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n"
     ]
    }
   ],
   "source": [
    "# Loop through each legislator and create a dataframe for each of them \n",
    "df_leader_list = []\n",
    "df_family_list = []\n",
    "df_list   = []\n",
    "\n",
    "for idx0, val0 in enumerate(within_legislator):\n",
    "    if idx0%500 == 0:\n",
    "        print(idx0)\n",
    "    for idx1, val1 in enumerate(val0):\n",
    "        subset = within_legislator[idx0][val1]\n",
    "        \n",
    "        if val1 == 'id:':\n",
    "            row_index = get_row_indices(subset, match='bioguide')\n",
    "        if val1 == 'name:':\n",
    "            row_index = get_row_indices(subset, match='first')\n",
    "        if val1 == 'bio:':\n",
    "            row_index = get_row_indices(subset, match='birthday')    \n",
    "        if val1 == 'terms:':\n",
    "            row_index = get_row_indices(subset, match='type')\n",
    "        \n",
    "        if val1 == 'leadership_roles:':\n",
    "            row_index = get_row_indices(subset, match='title')\n",
    "            temp = pd.DataFrame()\n",
    "            for idx2, val2 in enumerate(row_index):\n",
    "                temp.loc[val2, within_legislator[idx0]['id:'][0][0]] = within_legislator[idx0]['id:'][0][1]\n",
    "                temp.loc[val2, subset[idx2][0]] = subset[idx2][1]\n",
    "            df_leader_list.append(temp)\n",
    "        \n",
    "        elif val1 == 'family:':\n",
    "            row_index = get_row_indices(subset, match='name_fam')\n",
    "            temp = pd.DataFrame()\n",
    "            for idx2, val2 in enumerate(row_index):\n",
    "                temp.loc[val2, within_legislator[idx0]['id:'][0][0]] = within_legislator[idx0]['id:'][0][1]\n",
    "                temp.loc[val2, subset[idx2][0]] = subset[idx2][1]\n",
    "            df_family_list.append(temp)\n",
    "        \n",
    "        else:\n",
    "            if val1 == 'id:':\n",
    "                temp = pd.DataFrame()\n",
    "                for idx2, val2 in enumerate(row_index):\n",
    "                    temp.loc[val2, subset[idx2][0]] = subset[idx2][1]\n",
    "                df = temp\n",
    "            else:\n",
    "                temp = pd.DataFrame()\n",
    "                for idx2, val2 in enumerate(row_index):\n",
    "                    temp.loc[val2, within_legislator[idx0]['id:'][0][0]] = within_legislator[idx0]['id:'][0][1]\n",
    "                    temp.loc[val2, subset[idx2][0]] = subset[idx2][1]\n",
    "                df = df.merge(temp, on=['bioguide'])\n",
    "    \n",
    "    gender_match   = [re.match('gender',  x) is not None for x in list(df.columns)]\n",
    "    birthday_match = [re.match('birthday',x) is not None for x in list(df.columns)]\n",
    "    if sum(gender_match) == 1:\n",
    "        df['gender']   = df['gender'].fillna(method='ffill')\n",
    "        df['gender']   = df['gender'].fillna(method='bfill')\n",
    "    if sum(birthday_match) == 1:\n",
    "        df['birthday'] = df['birthday'].fillna(method='ffill')\n",
    "        df['birthday'] = df['birthday'].fillna(method='bfill')\n",
    "    \n",
    "    df.drop_duplicates(keep='first',inplace=True) \n",
    "\n",
    "    # append df to df-list\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df_family = pd.concat(df_family_list, ignore_index=True)\n",
    "df_leader = pd.concat(df_leader_list, ignore_index=True)\n",
    "\n",
    "df = df.drop(columns=['how'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df.to_csv('data/historical-legislators-US.csv', index=False,sep=',')\n",
    "df_family.to_csv('data/other/historical-legislators-family-US.csv', index=False,sep=',')\n",
    "df_leader.to_csv('data/other/historical-legislators-leader-US.csv', index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Legislators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yaml-file for historical data\n",
    "content = read_yaml(url=f\"{url}{current}\")\n",
    "content = clean_yaml(content, file='legislators')\n",
    "\n",
    "# Split list into sub-lists for each legislator\n",
    "between_legislator = split_list(match='id:', string=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure bioguide is on top and clean the family section\n",
    "for idx, val in enumerate(between_legislator):\n",
    "    clean_bioguide()\n",
    "    clean_family()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete contact information from list\n",
    "delete_info = [[re.match('address:|phone:|fax:|contact_form:|office:|other_names:|rss_url:',x) for x in l] for l in between_legislator]\n",
    "boolean = [[x is None for x in c] for c in delete_info]\n",
    "delete_indices = [get_index_positions(x, False) for x in boolean]\n",
    "between_legislator = [del_list_numpy(between_legislator[idx], delete_indices[idx]) for idx in range(len(between_legislator))]\n",
    "\n",
    "# Clean up fec and bioguide \n",
    "between_legislator = clean_bio(between_legislator, element='fec:')\n",
    "between_legislator = clean_bio(between_legislator, element='bioguide_previous:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a list for each type of information for each legislator.\n",
    "# I delete party affiliations within the 'terms'-list to avoid complications.\n",
    "#within_legislator = [split_list(match=\"id:|name:|bio:|terms:|leadership_roles:|family:\", string=x) for x in between_legislator]\n",
    "within_legislator = [split_list(match=\"id:|name:|bio:|terms:|leadership_roles:|family:\", string=x) for x in between_legislator]\n",
    "within_legislator = [{x[0]:x[1:] for x in l} for l in within_legislator]\n",
    "within_legislator = delete_party_affiliations(within_legislator)\n",
    "\n",
    "# Constuct column-row pairs for each legislator\n",
    "for idx0, val0 in enumerate(within_legislator):\n",
    "    for idx1, val1 in enumerate(val0):\n",
    "        within_legislator[idx0][val1] = [re.match('(.*): (.*)', x).groups() for x in within_legislator[idx0][val1]] \n",
    "    \n",
    "for idx in range(len(within_legislator)):\n",
    "    boolean_end = [re.match('end', x) is not None for x,y in within_legislator[idx]['name:']]\n",
    "    if sum(boolean_end) > 0:\n",
    "        end_position = get_index_positions(boolean_end, True)[0]\n",
    "        del within_legislator[idx]['name:'][end_position]\n",
    "\n",
    "for idx in range(len(within_legislator)):\n",
    "    boolean_middle = [re.match('~', y) is not None for x,y in within_legislator[idx]['name:']]\n",
    "    if sum(boolean_middle) > 0:\n",
    "        middle_position = get_index_positions(boolean_middle, True)[0]\n",
    "        del within_legislator[idx]['name:'][middle_position]\n",
    "\n",
    "for idx, val in enumerate(within_legislator):\n",
    "    last_positions = get_index_positions([re.match('last', x) is not None for x,y in within_legislator[idx]['name:']],True)\n",
    "    if len(last_positions) > 1:\n",
    "        max_last = max(last_positions)\n",
    "        del within_legislator[idx]['name:'][max_last]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Loop through each legislator and create a dataframe for each of them \n",
    "df_leader_list = []\n",
    "df_family_list = []\n",
    "df_list   = []\n",
    "\n",
    "for idx0, val0 in enumerate(within_legislator):\n",
    "    if idx0%100 == 0:\n",
    "        print(idx0)\n",
    "    for idx1, val1 in enumerate(val0):\n",
    "        subset = within_legislator[idx0][val1]\n",
    "        \n",
    "        if val1 == 'id:':\n",
    "            row_index = get_row_indices(subset, match='bioguide')\n",
    "        if val1 == 'name:':\n",
    "            row_index = get_row_indices(subset, match='first')\n",
    "        if val1 == 'bio:':\n",
    "            row_index = get_row_indices(subset, match='birthday|gender')    \n",
    "        if val1 == 'terms:':\n",
    "            row_index = get_row_indices(subset, match='type')\n",
    "        \n",
    "        if val1 == 'leadership_roles:':\n",
    "            row_index = get_row_indices(subset, match='title')\n",
    "            temp = pd.DataFrame()\n",
    "            for idx2, val2 in enumerate(row_index):\n",
    "                temp.loc[val2, within_legislator[idx0]['id:'][0][0]] = within_legislator[idx0]['id:'][0][1]\n",
    "                temp.loc[val2, subset[idx2][0]] = subset[idx2][1]\n",
    "            df_leader_list.append(temp)\n",
    "        \n",
    "        elif val1 == 'family:':\n",
    "            row_index = get_row_indices(subset, match='name_fam')\n",
    "            temp = pd.DataFrame()\n",
    "            for idx2, val2 in enumerate(row_index):\n",
    "                temp.loc[val2, within_legislator[idx0]['id:'][0][0]] = within_legislator[idx0]['id:'][0][1]\n",
    "                temp.loc[val2, subset[idx2][0]] = subset[idx2][1]\n",
    "            df_family_list.append(temp)\n",
    "        \n",
    "        else:\n",
    "            if val1 == 'id:':\n",
    "                temp = pd.DataFrame()\n",
    "                for idx2, val2 in enumerate(row_index):\n",
    "                    temp.loc[val2, subset[idx2][0]] = subset[idx2][1]\n",
    "                df = temp\n",
    "            else:\n",
    "                temp = pd.DataFrame()\n",
    "                for idx2, val2 in enumerate(row_index):\n",
    "                    temp.loc[val2, within_legislator[idx0]['id:'][0][0]] = within_legislator[idx0]['id:'][0][1]\n",
    "                    temp.loc[val2, subset[idx2][0]] = subset[idx2][1]\n",
    "                df = df.merge(temp, on=['bioguide'])\n",
    "                \n",
    "    df['gender']   = df['gender'].fillna(method='bfill')\n",
    "    df['gender']   = df['gender'].fillna(method='ffill')\n",
    "    df['birthday'] = df['birthday'].fillna(method='ffill')\n",
    "    df['birthday'] = df['birthday'].fillna(method='bfill')\n",
    "    df.drop_duplicates(keep='first',inplace=True) \n",
    "    \n",
    "    # append df to df-list\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df_family = pd.concat(df_family_list, ignore_index=True)\n",
    "df_leader = pd.concat(df_leader_list, ignore_index=True)\n",
    "\n",
    "df = df.drop(columns=['how'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df.to_csv('data/current-legislators-US.csv', index=False,sep=',')\n",
    "df_family.to_csv('data/other/current-legislators-family-US.csv', index=False,sep=',')\n",
    "df_leader.to_csv('data/other/current-legislators-leader-US.csv', index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yaml-file for historical data\n",
    "content = read_yaml(url=f\"{url}{social_media}\")\n",
    "content = clean_yaml(content, file='legislators')\n",
    "\n",
    "# Split list into sub-lists for each legislator\n",
    "between_legislator = split_list(match=\"id:\", string=content)\n",
    "\n",
    "# delete first element of list\n",
    "del between_legislator[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(between_legislator):\n",
    "    clean_bioguide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up fec and bioguide if needed to \n",
    "between_legislator = clean_bio(between_legislator, element='fec:')\n",
    "between_legislator = clean_bio(between_legislator, element='bioguide_previous:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_legislator = [split_list(match=\"id:|social:\", string=x) for x in between_legislator]\n",
    "within_legislator = [{x[0]:x[1:] for x in l} for l in within_legislator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx0, val0 in enumerate(within_legislator):\n",
    "    for idx1, val1 in enumerate(val0):\n",
    "        within_legislator[idx0][val1] = [re.match('(.*): (.*)', x).groups() for x in within_legislator[idx0][val1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for idx0, val0 in enumerate(within_legislator):\n",
    "    for idx1, val1 in enumerate(val0):\n",
    "            temp = pd.DataFrame(within_legislator[idx0][val1]).T\n",
    "            temp.columns = temp.iloc[0]\n",
    "            temp = temp[1:]\n",
    "            within_legislator[idx0][val1] = temp\n",
    "    df_list.append(within_legislator[idx0]['id:'].join(within_legislator[idx0]['social:']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df.to_csv('data/social-media-legislators-US.csv', index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = {\"lis\": str, \"thomas\":str ,\"ballotpedia\":str ,\"cspan\": str,\n",
    "            \"official_full\": str, \"opensecrets\": str, \"url\":str,\n",
    "            \"fec\": str, \"state_rank\": str, \"district\":str, \n",
    "            \"house_history\":str, \"votesmart\":str, \"govtrack\":str}\n",
    "\n",
    "df_historical = pd.read_csv('data/historical-legislators-US.csv', dtype=datatype)\n",
    "df_current    = pd.read_csv('data/current-legislators-US.csv',  dtype=datatype)\n",
    "\n",
    "datatype = {\"thomas\":str, \"govtrack\":str, \"twitter_id\":str,\n",
    "           'instagram_id':str, 'youtube_id':str, 'youtube':str,\n",
    "           'facebook':str, 'twitter':str, 'instagram': str}\n",
    "df_some  = pd.read_csv('data/social-media-legislators-US.csv', dtype=datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((df_historical, df_current))\n",
    "df = pd.merge(df, df_some, on=['bioguide'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['govtrack_y'], axis=1)\n",
    "df = df.rename(columns={'govtrack_x': 'govtrack'})\n",
    "\n",
    "df = df.drop(['thomas_y'], axis=1)\n",
    "df = df.rename(columns={'thomas_x': 'thomas'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/all-legislators-US.csv', index=False,sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
